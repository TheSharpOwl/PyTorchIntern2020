{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision \n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "train = datasets.MNIST(\"\",train = True, download = True,\n",
    "                       transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test = datasets.MNIST(\"\",train = False, download = True,\n",
    "                       transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size = 10, shuffle = True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size = 10, shuffle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will do some Neural Netowrks :D\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module): # remember this is inheritance\n",
    "    def __init__(self):\n",
    "        # many people forget this LOL\n",
    "        super().__init__()\n",
    "        #define the fully connected later (fc = full connected)\n",
    "        # 784 = 28 * 28 size of the flatened image, 64 is the output we just chose it so\n",
    "        self.fc1 = nn.Linear(784, 64) \n",
    "        # each later should contain input with the same size as the output of the previous one !\n",
    "        self.fc2 = nn.Linear(64, 64) \n",
    "        self.fc3 = nn.Linear(64, 64) \n",
    "        self.fc4 = nn.Linear(64, 10) \n",
    "    # And this is called Feed Forward NN because of the previous point\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # relu = rectified linear which is our activation function here\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim = 1)\n",
    "        \n",
    "        \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1879, -2.3063, -2.2774, -2.4064, -2.2441, -2.3160, -2.2817, -2.3081,\n",
       "         -2.3312, -2.3849]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's pass some data\n",
    "X = torch.rand((28,28))\n",
    "# we should change the demensions such that they would work with torch \n",
    "X = X.view(-1, 28*28) # we can put also 1 instead of 1\n",
    "output = net(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9112, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0961, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0029, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# The optimizer job is to adjust weights such that it lowers the loss\n",
    "\n",
    "# The 4th video starts here\n",
    "import torch.optim as optim\n",
    "# net.parameters are the adjustable parameters in our net, lr = learning rate \n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        # data is a batch of feature sets and labels\n",
    "        X, y = data # X is the something like the pixel values (like gray scale) and y is the label or the class\n",
    "        net.zero_grad()\n",
    "        output = net(X.view(-1, 28 * 28))\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurancy :  0.98\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0 \n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1,784))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            \n",
    "            \n",
    "print(\"Accurancy : \", round(correct/total, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X[1].view(28,28))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7, grad_fn=<NotImplemented>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(net(X[1].view(-1,784))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
